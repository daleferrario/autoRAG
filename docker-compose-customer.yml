services:
  anythingllm:
      image: mintplexlabs/anythingllm
      container_name: anythingllm.${CUSTOMER_ID}
      ports:
      - "3001:3001"
      cap_add:
        - SYS_ADMIN
      environment:
      # Server settings
        - SERVER_PORT=3001
        - STORAGE_DIR="/app/server/storage"
        - UID='1000'
        - GID='1000'
      # Core LLM
        - LLM_PROVIDER=ollama
        - OLLAMA_BASE_PATH=http://ollama:11434
        - OLLAMA_MODEL_PREF=llama3:latest
        - OLLAMA_MODEL_TOKEN_LIMIT=4096
      # Embedding
        - EMBEDDING_ENGINE=ollama
        - EMBEDDING_BASE_PATH=http://ollama:11434
        - EMBEDDING_MODEL_PREF=mxbai-embed-large:latest
        - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192
      # # Vector DB
      #   - VECTOR_DB="chroma"
      #   - CHROMA_ENDPOINT='http://chromadb:8000'
      volumes:
        - anythingllm_storage:/app/server/storage
      networks:
      - distill_network
      restart: always

  # anythingllm_storage_setup:
  #       image: alpine:latest
  #       container_name: anythingllm_storage_setup
  #       command: >
  #         sh -c "
  #           while ! curl -s http://ollama:11434/api/list > /dev/null; do
  #             echo 'Waiting for ollama server...'
  #             sleep 1
  #           done &&
  #           curl -X POST http://ollama:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\": \"llama3\"}' &&
  #           curl -X POST http://ollama:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\": \"mxbai-embed-large\"}'"
  #       restart: "no"
  #       volumes:
  #       - anythingllm_storage:/app/server/storage
  #       networks:
  #         - distill_network
  #       depends_on:

volumes:
  anythingllm_storage:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /home/ubuntu/anythingllm_storage

networks:
  distill_network:
    external: true